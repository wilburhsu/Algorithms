[toc]
## 1.Java集合

### ArrayList、LinkedList

ArrayList：数组实现，自动扩容1.5倍

LinkedList：双向链表实现，带有头结点和尾结点，头插和尾插性能好

### HashMap

put方法过程：

0. 判断是否需要扩容

1. key值计算hash，得到32位hashcode

2. 将得到的hashcode右移16位取得高16位
3. 将前两步得到的值进行异或运算得到最终的hash值
4. 然后根据数组长度进行取模计算， `(n - 1) & hash` ，得到在数组中的 index 下标
5. 索引位置为空，建立新的k-v节点；如果不为空，先判断已存在的key值的hash与要插入的是否相同，相同覆盖，不相同以**链表**形式连接到当前key值之后

### ConcurrentHashMap

分段锁

### CopyOnWriteArrayList

`CopyOnWriteArrayList` 是 `ArrayList` 的线程安全版本

1. 写操作：使用了一种叫**写时复制**的方法，当有新元素添加到`CopyOnWriteArrayList`时，先从原有的数组中拷贝一份出来，然后在新的数组做写操作，写完之后，再将原来的数组引用指向到新数组。整个`add()`操作都是在锁的保护下进行，避免在多线程并发`add()`的时候，复制出多个副本，导致最终的数组数据不是期望的
2. 读操作：读操作不加锁。如果有线程并发的读，则分几种情况： 
   1. 如果写操作未完成，那么直接读取原数组的数据
   2. 如果写操作完成，但是引用还未指向新数组，那么也是读取原数组数据
   3. 如果写操作完成，并且引用已经指向了新的数组，那么直接从新数组中读取数据

## 2.并发

并发编程的3个基本特性：原子性，可见性，有序性。

### volatile关键字

**作用：**

`volatile`是轻量级的`synchronized`，保障可见性、保障有序性和保障long/double类型变量读写操作的原子性

1. 保障原子性：保障long/double类型变量读写操作的原子性（Java对除long和double之外的基本类型的写操作都是原子性的，但是32位虚拟机上对这两种变量的写操作不具备原子性，要保证原子性需要用volatile进行修饰）

2. 保障有序性：提示JIT编译器被修饰的变量可能被多个线程共享，以阻止JIT编译器做出可能导致程序运行不正常的优化

3. 保障可见性：

   1. 读取一个volatile关键字修饰的变量会使相应的处理器执行**刷新处理器缓存**（从其他处理器的高速缓存或者主内存中对更新的共享变量进行缓存同步）的动作
   2. 写一个volatile关键字修饰的变量会使相应的处理器执行**冲刷处理器缓存**（将对共享变量的更新从写缓冲器写入到高速缓存或者主内存中）的工作

   两者结合保障了可见性。

**应用场景：**

1. 使用`volatile`变量作为状态标志
2. 使用`volatile`保证可见性
3. 在有限的一些情形下使用`volatile`变量替代锁
4. 使用`volatile`实现简易版读写锁（混合使用锁和volatile变量，锁保障原子性，volatile保障可见性）

**底层原理：**

`volatile` 的底层实现原理是内存屏障，内存屏障提供3个功能：

1. 确保指令重排序时不会把内存屏障前后的指令交换顺序，在执行到内存屏障这句指令时，它前面的操作已经全部完成
2. 它会强制将对缓存的修改操作立即写入主存；
3. 如果是写操作，它会导致其他CPU中对应的缓存行无效

可见性的实现：

1. 对 `volatile` 变量的写指令后会加入写屏障，写屏障（sfence）保证在该屏障之前的，对共享变量的改动，都同步到主存当中 
2. 对 `volatile` 变量的读指令前会加入读屏障 ，读屏障（lfence）保证在该屏障之后，对共享变量的读取，加载的是主存中最新数据  

有序性的实现：

1. 写屏障会确保指令重排序时，不会将写屏障之前的代码排在写屏障之后
2. 读屏障会确保指令重排序时，不会将读屏障之后的代码排在读屏障之前 

更底层是读写变量时使用 lock 指令来保证多核 CPU 之间的可见性与有序性  

### synchronized关键字

 `synchronized` 关键字，有以下三种使用方式:

- **同步普通方法**，锁的是当前对象。
- **同步静态方法**，锁的是当前 `Class` 对象。
- **同步块**，锁的是 `()` 中的对象。

实现原理： `JVM` 通过进入、退出对象监视器( `Monitor` )来实现对方法、同步块的同步。

1. synchronized 同步语句块的情况

   synchronized 同步语句块的实现使用的是 monitor.enter 和 monitor.exit 指令。

   monitor.enter 指令指向同步代码块的开始位置，monitor.exit 指令则指明同步代码块的结束位置。本质就是对一个对象监视器( `Monitor` )进行获取，而这个获取过程具有排他性从而达到了同一时刻只能一个线程访问的目的。

   执行 monitor.enter 指令时，线程试图获取锁也就是获取 monitor（monitor对象存在于每个Java对象的对象头中）的持有权。

   当计数器为0则可以成功获取，获取后将锁计数器加1。在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放

2. synchronized 修饰方法的的情况

   JVM 通过访问ACC_SYNCHRONIZED 方法标识符来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。

==synchronized 锁的优化==

轻量级锁，偏向锁，自旋锁，锁膨胀

### Java中的锁

1. `synchronized`关键字

2. `java.util.concurrent.locks`包下常用的类

   1. `Lock`接口

      `ReentrantLock`：可重入锁，`Lock`接口的一个实现，继承AQS，并且提供了更多的方法

   2. `ReadWriteLock`接口

      `ReentrantReadWriteLock`：可重入的读写锁，实现了`ReadWriteLock`接口，最主要的有两个方法：`readLock()`和`writeLock()`用来获取读锁和写锁

   3. `StampedLock`：JDK8新增

3. ==Java中如何解决死锁==

   1. **破坏互斥条件** ：这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。
   2. **破坏请求与保持条件** ：一次性申请所有的资源。
   3. **破坏不剥夺条件** ：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。
   4. **破坏循环等待条件** ：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。

4. 乐观锁和悲观锁

   1. 悲观锁
      1. 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）
      2. 传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中`synchronized`和`ReentrantLock`等独占锁就是悲观锁思想的实现
   2. 乐观锁
      1. 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现
      2. 乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于`write_condition`机制，其实都是提供的乐观锁。Java中`java.util.concurrent.atomic`包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的

5. 公平锁和非公平锁：

   ReentrantLock 默认采用非公平锁，因为考虑获得更好的性能，通过 boolean 来决定是否用公平锁（传入 true 用公平锁）

   公平锁和非公平锁只有两处不同：

   1. 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。
   2. 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。

   公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。

   相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。

### AQS

AQS全称是 AbstractQueuedSynchronizer，是阻塞式锁和相关的同步器工具的框架。

常见基于AQS的同步器：CyclicBarrier，CountdownLatch，Semaphore，ReentrantLock，ReentrantReadWriteLock

原理：用volatile修饰共享变量state，线程通过对state的CAS操作来获取锁和解锁，成功则获取锁成功，失败则park相关线程之后进入等待队列或条件等待队等待，有公平和非公平两种模式来唤醒等待的线程。使用AQS是为了封装和抽象，通过封装公共的方法，减少代码重复。

AQS 的基本思想如下：

获取锁的逻辑

```
while(state 状态不允许获取) {
	if(队列中还没有此线程) {
		入队并阻塞
	}
}
当前线程出队
```

释放锁的逻辑

```
if(state 状态允许了) {
	恢复阻塞的线程(s)
}
```

### volatile和synchronized区别

- `volatile`关键字是线程同步的轻量级实现，`volatile`不具有排他性，也不会导致上下文切换，所以`volatile`性能比`synchronized`关键字要好
- 但是`volatile`关键字只能用于变量，而`synchronized`关键字可以修饰方法以及代码块
- 多线程访问`volatile`关键字不会发生阻塞，而`synchronized`关键字可能会发生阻塞
- `volatile`关键字能保证数据的可见性，但不能保证数据的原子性。`synchronized`关键字两者都能保证
- `volatile`关键字主要用于解决变量在多个线程之间的可见性，而`synchronized`关键字解决的是多个线程之间访问资源的同步性

### synchronized和lock区别

1.  两者都是可重入锁
   - 都是加锁方式同步，而且都是阻塞式的同步
   - 两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。同一个线程每次获取锁，锁的计数器都自增1，要等到锁的计数器下降为0时才能释放锁。

2.  `synchronized` 依赖于 JVM 而 `ReentrantLock` 依赖于 API
   - `synchronized`是java语言的关键字，是原生语法层面的互斥，是依赖于 JVM 实现的
   - `ReentrantLock` 是 JDK 层面实现的，需要 `lock() `和` unlock() `方法配合` try`/`finally` 语句块来完成

3. 在锁的细粒度和灵活度和方面，`ReentrantLock`优于`synchronized`
   - `synchronized`由编译器去保证锁的加锁和释放
   - `ReenTrantLock`需要手工声明来加锁和释放锁

4. `ReentrantLock` 比 `synchronized` 增加了一些高级功能
   - `ReentrantLock`通过`lock.lockInterruptibly()`提供了一种能够中断等待锁的线程的机制
   - `ReentrantLock`可以指定是公平锁还是非公平锁，`synchronized`只能是非公平锁。通过 `ReentrantLock`类的`ReentrantLock(boolean fair)`构造方法来指定
   - 等待/通知机制：`synchronized`关键字与`wait()`和`notify()`/`notifyAll()`方法相结合实现，`ReentrantLock`类借助于`Condition`接口与`newCondition() `方法实现

### 线程池

[线程池总结](https://gitee.com/SnailClimb/JavaGuide/blob/master/docs/java/multi-thread/java线程池学习总结.md)

Executor  -> ExecutorService -> **ThreadPoolExecutor**

FixedThreadPool、SingleThreadExecutor、CachedThreadPool可用Executor创建，都是调用了 ThreadPoolExecutor 的构造方法

ScheduledThreadPoolExecutor 继承了 ThreadPoolExecutor 并实现了 ScheduledExecutorService 

<img src="http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/1719C1F2396B4EA8AD473D41C7BA664D/11708" alt="任务的执行相关接口" style="zoom:50%;" />

#### 线程池参数

```java
new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, milliseconds,runnableTaskQueue, handler);
```

1. `corePoolSize`：线程池的基本大小。当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程。如果调用了线程池的`prestartAllCoreThreads()`方法，线程池会提前创建并启动所有基本线程
2. `maximumPoolSize`：线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。使用无界队列此参数无效。
3. `keepAliveTime`：空闲线程存活时间。一个线程如果处于空闲状态，并且当前的线程数量大于corePoolSize，那么在keepAliveTime时间后，空闲线程会被销毁。
4. `milliseconds`：`TimeUnit`枚举类型的值，代表`keepAliveTime`时间单位
5. `runnableTaskQueue`：任务队列。用于保存等待执行的任务的阻塞队列。可以选择以下几个阻塞队列
   - `ArrayBlockingQueue`：基于数组结构的有界阻塞队列
   - `LinkedBlockingQueue`：基于链表结构的阻塞队列。
   - `SynchronousQueue`：不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态
   - `PriorityBlockingQueue`：具有优先级的无限阻塞队列

6. `RejectedExecutionHandler`：拒绝策略。

#### 线程池原理

`ThreadPoolExecutor`执行`execute()`方法分4步：

1. 如果当前运行的线程少于`corePoolSize`，则创建新线程来执行任务（需要获取全局锁）
2. 如果运行的线程等于或多于`corePoolSize`，则将任务加入`BlockingQueue`
3. 如果无法将任务加入`BlockingQueue`（队列已满），则创建新的线程来处理任务（需要获取全局锁）
4. 如果创建新线程将使当前运行的线程超出`maximumPoolSize`，任务将被拒绝，并调用`RejectedExecutionHandler.rejectedExecution()`方法

`ThreadPoolExecutor`采取上述步骤的总体设计思路，是为了在执行`execute()`方法时，尽可能地避免获取全局锁（那将会是一个严重的可伸缩瓶颈）。在`ThreadPoolExecutor`完成预热之后（当前运行的线程数大于等于`corePoolSize`），几乎所有的`execute()`方法调用都是执行步骤2，而步骤2不需要获取全局锁

线程池处理流程图：

<img src="https://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/1938A70E46EA414AB7B7244FB2267EA1/5547" alt="image" style="zoom:67%;" />

#### 线程池的拒绝策略

ThreadPoolExecutor.AbortPolicy：丢弃任务并抛出RejectedExecutionException异常（默认策略）
ThreadPoolExecutor.DiscardPolicy：丢弃任务，但是不抛出异常
ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新提交被拒绝的任务
ThreadPoolExecutor.CallerRunsPolicy：由调用线程（提交任务的线程）处理该任务

### CyclicBarrrier和CountdownLatch

1. CountDownLatch的实现是基于AQS的，而CycliBarrier是基于 ReentrantLock(ReentrantLock也属于AQS同步器)和 Condition 的
2. CountDownLatch 是一个线程等待其他线程， CyclicBarrier 是多个线程互相等待
3. 调用CountDownLatch的countDown方法后，当前线程并不会阻塞，会继续往下执行；而调用CyclicBarrier的`await`方法，会阻塞当前线程
4. CountDownLatch只能使用过一次，CyclicBarrier在调用reset之后还可以继续使用

## 3.NIO

SelectorKey

Channel

ByteBuffer

## 4.JVM

### 工作原理

Java源文件经过前端编译器（javac或ECJ）将.java文件（保存在硬盘中）编译为.class文件（Java字节码文件），然后JRE加载器从硬盘中读取Java字节码文件，载入系统分配给JVM的内存区域--运行数据区（Runtime Data Areas），然后执行引擎解释或编译类文件，再由即时编译器将字节码转化为特定CPU的机器码，CPU执行机器码。

<img src="http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/9A86BFE070D44B85B8F033716788C5B9/11804" style="zoom:70%;" />

### 内存模型

线程共有，线程私有

![](http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/2A010FC6679A4C059F4CE6B26522FB06/11546)

#### Java堆内存分代管理

1. 新生代：

   - Eden空间：对象优先在Eden分配，空间不足时，虚拟机将发起一次Minor GC

   - From Survivor空间、To Survivor空间：在Minor GC时交替使用，达到一定次数后，对象会晋升到老年代。如果Minor GC后仍存活的对象无法放入Survivor，则通过分配担保机制提前将对象转移到老年代

     > 空间分配担保机制：老年代的连续空间大于新生代对象的总大小或者历次晋升到老年代的对象的平均大小就进行`MinorGC`，否则`FullGC`。
     >
     > JDK6 update 24之前通过设置-XX:HandlePromotionFailure参数生效。如果HandlePromotionFailure=true，那么会继续检查老年代最大可用连续空间是否大于**历次晋升到老年代的对象的平均大小**，如果大于，则尝试进行一次Minor GC，但这次Minor GC依然是有风险的；如果小于或者HandlePromotionFailure=false，则改为进行一次Full GC。
     
     > ***为什么需要两个Survivor区*：**
     >
     > 设置两个Survivor区最大的好处就是解决了碎片化。
     >
     > 1. 如果只有一块Survivor区。Eden满了，第一次触发Minor GC，Eden中的存活对象被移动到Survivor区；再次触发Minor GC，Eden和Survivor各有一些存活对象： 
     >
     >    - 若此时把Eden区的存活对象直接放到Survivor区，这两部分对象所占有的内存是不连续的，导致了内存碎片化，严重影响Java程序的性能
   >
     >    - 若整理Survivor区后再放入Eden区的存活对象，则效率较低
     >
     > 2. 如果有两块Survivor区。第一次触发Minor GC，Eden中的存活对象就会被移动到第一块Survivor区S0，Eden被清空；再次触发Minor GC时，Eden和S0中的存活对象又会被复制送入第二块survivor区S1，S0和Eden被清空；然后下一轮S0与S1交换角色，如此循环往复。整个过程中，永远有一个Survivor区是空的，另一个非空的Survivor区无碎片
   
     > ***Eden区和Survivor区分配的大小比例：***
     >
     > 新生代中的对象98%是“朝生夕死”的，HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。

2. 老年代：大对象（需要大量连续内存空间）直接进入老年代；长期存活的对象进入老年代，对象每“熬过”一次Minor GC年龄增加一岁，到一定程度（默认为15岁）则晋升到老年代

   > GC分代年龄为什么默认为15：Java对象头中用4bit保存分代年龄，4bit能表示的最大二进制为1111，即十进制的15
   >
   > 对象进入老年代的4个常见时机：
   >
   > 1. 躲过15次gc，达到15岁高龄之后进入老年代；
   > 2. **动态年龄判定规则**，如果Survivor区域内年龄1+年龄2+年龄3+···+年龄n的对象总和大于Survivor区的50%，此时年龄n以上的对象会进入老年代，不一定要达到15岁
   > 3. 如果一次Young GC后存活对象太多无法放入Survivor区，此时直接进入老年代
   > 4. 大对象直接进入老年代

3. 永久代：存储类定义、结构、字段、方法（数据及代码）以及常量在内的类相关数据

   > JDK 1.8中永久代被元空间（Metaspace）取代。两者本质类似，都是对JVM规范中方法区的实现，最大区别是：永久代的大小很难确定，对永久代的调优过程非常困难；元空间并不在虚拟机中，而是使用本地内存，最大可分配空间就是系统可用内存空间

> 是否所有的对象和数组都会在堆上分配内存（**逃逸分析**）
>
> 在Java虚拟机中，对象是在Java堆中分配内存的，这是一个普遍的常识。但是，有一种特殊情况，那就是如果经过逃逸分析后发现，一个对象并没有逃逸出方法的话，那么就可能被优化成栈上分配。这样就无需在堆上分配内存，也无须进行垃圾回收了。

### 垃圾回收

![](http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/12E1F654FFC743DB92996C0B7F9AA7EC/11552)

#### 垃圾回收策略

1. **引用计数法**：给对象中添加一个引用计数器，每当有一个地方引用它时，计数器就加1，当引用失效时，计数器就减1；任何时刻计数器为0的对象就是不可能在被使用的

   主流的Java虚拟机里面没有选择引用计数算法来管理内存，其中主要的原因是它很难解决对象之间的相互循环引用的问题

2. **可达性分析法**：在主流的商用程序语言中使用。基本思路就是通过一系列的名为GC Roots的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连（对象不可达）时，则证明此对象是不可用的

   > Java中可以作为GC Roots的对象：
   >
   > 1. 虚拟机栈（栈帧中的本地变量表）中引用的对象
   > 2. 方法区中类静态属性引用的对象
   > 3. 方法区中常量引用的对象
   > 4. 本地方法栈中JNI（即一般说的native方法）中引用的对象

#### 垃圾回收算法

标记-清除算法，复制算法，标记-整理算法，分代收集算法

1. 标记-清除算法：**回收老年代**。最基础的收集算法，分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，标记完成后统一回收所有被标记的对象

   主要不足有两个：

   - **效率问题**，标记和清除两个过程的效率都不高；
   - **空间问题**，标记清除之后会产生大量不连续的内存碎片

2. 复制算法：**回收新生代**。为了解决效率问题，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。

3. 标记-整理算法：**回收老年代**。标记过程仍然与“标记-清除”算法一样，标记完成后让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存

4. 分代收集算法：当前商业虚拟机的垃圾收集都采用“分代收集”算法，根据对象存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法

   - 在新生代中选用复制算法
   - 老年代中使用“标记—清理”或者“标记—整理”算法来进行回收

#### 垃圾收集器

JDK1.6默认垃圾收集器：新生代ParNew，老年代Serial Old（一般使用CMS代替）

JDK1.8默认垃圾收集器：新生代Parallel Scavenge，老年代Parallel Old

| 收集器            | 执行方式 | 适用分代 | 算法               | 目标         | 适用场景                                  |
| ----------------- | -------- | -------- | ------------------ | ------------ | ----------------------------------------- |
| Serial            | 串行     | 新生代   | 复制算法           | 响应速度优先 | 单CPU环境下的Client模式                   |
| Serial Old        | 串行     | 老年代   | 标记-整理          | 响应速度优先 | 单CPU环境下的Client模式、CMS的后备预案    |
| ParNew            | 并行     | 新生代   | 复制算法           | 响应速度优先 | 多CPU环境时在Server模式下与CMS配合        |
| Parallel Scavenge | 并行     | 新生代   | 复制算法           | 吞吐量优先   | 在后台运算而不需要太多交互的任务          |
| Parallel Old      | 并行     | 老年代   | 标记-整理          | 吞吐量优先   | 在后台运算而不需要太多交互的任务          |
| CMS               | 并发     | 老年代   | 标记-清除          | 响应速度优先 | 集中在互联网站或B/S系统服务端上的Java应用 |
| G1                | 并发     | both     | 标记-整理+复制算法 | 响应速度优先 | 面向服务端应用，将来替换CMS               |

##### CMS收集器

CMS 收集器是一种 **“标记-清除”算法**实现的，整个过程分为四个步骤：

- **初始标记：** 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快
- **并发标记：** 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。
- **重新标记：** 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短
- **并发清除：** 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。

初始标记和重新标记会Stop The World；并发标记和并发清除耗时最长

**CMS使用过程会出现的问题：产生内存碎片，产生浮动垃圾**

在并发清理期间，系统程序可能先把某些对象分配在新生代，然后可能触发了一次MinorGC，一些对象进入了老年代，然后短时间内又没人引用这些对象了。这种对象，就是老年代的“**浮动垃圾**”。  这些对象虽然成为了垃圾，但是CMS只能回收之前标记出来的垃圾对象，不会回收他们，需要等到下一次GC的时候才会回收他们。  

为了保证在CMS垃圾回收期间，还有一定的内存空间让一些对象可以进入老年代，一般会预留一些空间。CMS垃圾回收的触发时机，其中有一个就是当老年代内存占用达到一定比例了，就自动执行GC。“-XX:CMSInitiatingOccupancyFaction”参数可以用来设置老年代占用多少比例的时候触发CMS垃圾回收，JDK 1.6里面默认的值是92%。也就是说，老年代占用了92%空间了，就自动进行CMS垃圾回收，预留8%的空间给并发回收期间，系统程序把一些新对象放入老年代中。

如果CMS垃圾回收期间，系统程序要放入老年代的对象大于了可用内存空间，会发生Concurrent Mode Failure，即并发垃圾回收失败了，我一边回收，你一边把对象放入老年代，内存都不够了。此时就会自动用“Serial Old”垃圾回收器替代CMS，就是直接强行把系统程序“Stop the World”，重新进行长时间的GC Roots追踪，标记出来全部垃圾对象，不允许新的对象产生然后一次性把垃圾对象都回收掉，完事儿了再恢复系统线程。所以在生产实践中，这个自动触发CMS垃圾回收的比例需要合理优化一下，避免“Concurrent Mode Failure”问题  

CMS不是完全仅仅用“标记-清理”算法，CMS有一个参数是“-XX:+UseCMSCompactAtFullCollection”，默认打开，作用是在Full GC之后要再次进行“Stop the World”，停止工作线程，然后进行碎片整理，就是把存活对象挪到一起，空出来大片连续内存空间，避免内存碎片。  还有一个参数是“-XX:CMSFullGCsBeforeCompaction”，作用是执行多少次Full GC之后再执行一次内存碎片整理的工作，默认是0，意思就是每次Full GC之后都会进行一次内存整理。

##### G1收集器

G1 收集器的运作大致分为以下几个步骤：初始标记、并发标记、最终标记、筛选回收，使用 Region 划分内存空间以及有优先级的区域回收方式

#### GC调优

**GC调优的目的：减少GC时间和GC频率**

GC调优一般具体是通过GC日志的情况来分析。

1. 发现minor gc频繁，新生代空间太小了。
2. 如果发现晋升的年龄很小，老年代迅速被填满，导致频繁的major gc，并且回收比率又很大，那说明对象的生命周期确实很短也需要调整新生代。
3. 如果看full gc很频繁，但是每次回收的内存就一点点，那目测就是内存泄露了。

总体上就是根据分代的根本，也就是新生代朝生夕死的事实调整GC，避免分配大对象。具体还是得分析GC日志。

**内存泄漏排查**

Java的内存泄露（Memory Leak）多半是因为对象存在无效的引用，对象得不到释放，如果发现Java应用程序占用的内存出现了泄露的迹象，那么一般采用下面的步骤分析：

1. 用工具生成java应用程序的heap dump（如jmap）
2. 使用Java heap分析工具（如MAT），找出内存占用超出预期的嫌疑对象
3. 根据情况，分析嫌疑对象和其他对象的引用关系
4. 分析程序的源代码，找出嫌疑对象数量过多的原因

**ParNew+CMS的gc，如何保证只做ygc，jvm参数如何配置**

> **一次Full GC的发生过程**
>
> 1. 在新生代Eden区填满后，首先会检查老年代的可用空间是否大于新生代全部对象。如果大于，即一次Minor GC过后，全部对象存活，老年代也能放得下，那么此时就会直接执行Minor GC
>    - Minor GC后存活的对象如果能全部放入Survivor区则进入Survivor区（之后不会发生Full GC）
>    - 如果放不下则通过空间担保机制直接进入老年代
> 2. 在上述步骤进行数次Minor GC后，老年代空间逐渐被存活对象填充（第1步中第2种情况）。此时新生代Eden区又被填满，老年代可用空间不足以存放新生代全部对象，如果“-XX:-HandlePromotionFailure”参数被打开（一般都会打开），此时会进入第二步检查，看老年代可用空间是否大于历次Minor GC过后进入老年代的对象的平均大小（动态对象年龄判定）。
>    - 在检查之后，发现老年代的可用空间大于历次Minor GC过后进入老年代的对象的平均大小，此时仍然再进行Minor GC
>    - 如果老年代的空间小于历次Minor GC过后进入老年代的对象的平均大小，进行第3步
>
> 3. 在第2步又进行数次Minor GC后，新生代Eden区又被填满。执行Minor GC之前进行检查，此时发现老年代的空间小于历次Minor GC过后进入老年代的对象的平均大小，此时就会直接触发一次Full GC。Full GC会把老年代的垃圾对象都给回收，接着执行Minor GC，然后将此次Minor GC后新生代中存活的对象转移到老年代
>
> **避免Full GC**
>
> 1. 保证老年代可用空间大于新生代所有对象，避免MinorGC前进行FullGC
> 2. 如果1可以保证，那后面-XX：HandlePromotionFailure、进入老年代的对象平均大小等比较就不需要考虑了
> 3. 保证MinorGC后存活对象不大于Survivor空间
>
> **避免年轻代对象进入老年代**
>
> 1. 根据实际情况查看每次MinorGC后存活对象的大小，设置合适的Survivor区域大小，保证存活对象进入survivor
>    区，而不是进入老年代
> 2. 根据对象存活的时间以及MinorGC的间隔时间，确定年龄。比如：3分钟一次MinorGC，而对象可以存活1个小
>    时，那就把对象年龄设置到20，避免对象15岁进入老年代
> 3. 大对象如果偶尔创建一个，可以设置-XX:PretenureSizeThreshold，使其分配至年轻代。如果创建销毁频繁，就
>    让其直接进入老年代，利用对象池避免频繁创建销毁  

### HotSpot 虚拟机对象

1. **对象的创建过程**

   ![](http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/8B831B370092496A8A3E21A22CC88A1B/11724)

   **Step1:类加载检查**：虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。

   **Step2:分配内存**：**分配方式**有 **“指针碰撞”** （堆规整的情况）和 **“空闲列表”**（堆不规整的情况） 两种

   虚拟机采用两种方式来保证创建对象时的线程安全：

   - **CAS+失败重试：** CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。**虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。**
   - **TLAB：**为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配

   **Step3:初始化零值：**内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头）

   **Step4:设置对象头**：虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。

   **Step5:执行 init 方法**：一般来说，执行 new 指令之后会接着执行 `<init>` 方法，这样一个真正可用的对象才算完全产生出来

2. **对象的内存布局**

   对象在内存中的布局可以分为 3 块区域：**对象头**、**实例数据**和**对齐填充**

   **对象头包括两部分信息**，**第一部分用于存储对象自身的运行时数据**（哈希码、GC 分代年龄、锁状态标志等等），**另一部分是类型指针**，即对象指向它的类元数据的指针

   **实例数据部分是对象真正存储的有效信息**，也是在程序中所定义的各种类型的字段内容

   **对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用**

3. **对象的访问定位**

   建立对象就是为了使用对象， Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有**①使用句柄**和**②直接指针**两种：

   1. **句柄：**如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息；
   2. **直接指针：** 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址。

   **使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。**

### 类加载机制

****

**双亲委派模式**：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此

<img src="https://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/9B449FCD76974551A97D9A6AFA0975B3/5557" alt="image" style="zoom:67%;" />

**类的加载**：将编译好的class类文件中的字节码读入到内存中，将其放在方法区内创建对应的class对象，类的加载分为：加载、验证、准备、解析、初始化。加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的。

![image](https://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/F5D9C6FC016743B491D80D4310F63989/5559)

## 5.框架

### Spring

#### IoC

#### AOP

#### 动态代理

JDK动态代理

Cglib动态代理

#### ==Spring如何解决循环依赖==

#### Spring Bean生命周期

![](http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/8E6E7B9664CA475EA1961D36C734E0C9/11548)

### Spring Boot

> 1.提供了一个配置类，该配置类定义了我们需要的对象的实例化过程；
> 2.提供了一个spring.factories文件，包含了配置类的全限定名；
> 3.将配置类和spring.factories文件打包为一个启动器starter；
> 4.程序启动时通过加载starter.jar包的spring.factories文件信息，然后通过反射实例化文件里面的类。

#### Spring Boot的启动过程

Spring Boot 在启动的时候，按照约定去读取 Spring Boot Starter 的配置信息，再根据配置信息对资源进行初始化，并注入到 Spring 容器中。这样 Spring Boot 启动完毕后，就已经准备好了一切资源，使用过程中直接注入对应 Bean 资源即可。

1. Spring Boot 在启动时会自动去读取依赖的每个starter 包中的 resources/META-INF/spring.factories 文件，该文件里配置了spring容器中所有需要的bean，然后根据文件中配置的 Jar 包去扫描项目所依赖的 Jar 包。

2. 根据 spring.factories 配置加载 AutoConfigure 类

3. 根据 @Conditional 注解的条件，进行自动配置并将 Bean 注入 Spring Context

#### Spring Boot的自动装配机制

Spring Boot 项目的启动注解是：`@SpringBootApplication`，它由下面三个注解组成：

- `@EnableAutoConfiguration` : 借助`@Import`的帮助，将所有符合自动配置条件的bean定义加载到IoC容器。
- `@ComponentScan` : 扫描被`@Component` (`@Service`,`@Controller`)注解的 bean，注解默认会扫描该类所在的包下所有的类
- `@Configuration` : 允许在 Spring 上下文中注册额外的 bean 或导入其他配置类

其中 `@EnableAutoConfiguration` 是实现自动配置的入口，该注解通过Spring 提供的 `@Import` 注解导入了`AutoConfigurationImportSelector`类（`@Import` 注解可以导入配置类或者Bean到当前类中），在该类中加载 META-INF/spring.factories 的配置信息。`AutoConfigurationImportSelector`类中`getCandidateConfigurations`方法会筛选出所有以 `EnableAutoConfiguration` 为 key 的数据以 List 的形式返回，加载到Spring IoC 容器中作 bean 来管理，实现自动配置功能

### ==MyBatis==

## 6.数据库（MySQL）

### 数据库架构

#### SQL执行过程 

![](http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/DE5B0357E4124930A9A7476389E1FC5C/11545)

#### B+树

B树和B+树的区别：

1. 在B树中，可以将键和值存放在内部节点和叶子节点，但在B+树中，内部节点都是键，没有值。叶子节点同时存放键和值
2. B+树的叶子节点有一条链相连，而B树的叶子节点各自独立。

在查询数据时：

1. 由于B+树的内部节点只存放键，不存放值，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。
2. B+树的叶节点由一条链相连，因此，当需要进行一次全数据遍历的时候，B+树只需要使用O(logN)时间找到最小的一个节点，然后通过链进行O(N)的顺序遍历即可。而B树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间

B树在提高了IO性能的同时并没有解决元素遍历的效率低下的问题，正是为了解决这个问题，B+树应用而生。B+树只需要去遍历叶子节点就可以实现整棵树的遍历

### SQL优化及索引

#### SQL优化思路

一个 SQL 执行的很慢，要分两种情况讨论：

1. 大多数情况下很正常，偶尔很慢，则有如下原因

   (1) 数据库在刷新脏页，例如 redo log 写满了需要同步到磁盘。**解决方法：根据主机的IO能力设置innodb_io_capacity（默认为0）参数**

   (2) 执行的时候，遇到锁，如表锁、行锁。**用`show processlist` 命令查看是否在等待锁，等待锁释放**

2. 这条 SQL 语句一直执行的很慢，则有如下原因。

   (1) 没有用上索引：例如该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。

   (2) 数据库选错了索引。索引系统是通过遍历部分数据，也就是通过**采样**的方式，来预测索引的基数的。采样有可能出现**失误**的情况。解决的方法可以通过强制走索引的方式来查询。也可以通过

   ```sql
   show index from t;
   ```

   来查询索引的基数和实际是否符合，如果和实际很不符合的话，可以重新来统计索引的基数，可以用下面这条命令来重新统计分析

   ```sql
   analyze table t;
   ```

   <img src="http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/47958803FD094550A1C5FB9DF671FD11/11683" style="zoom:50%;" />

#### limit优化

1. 避免请求不必要的数据：避免 `select *` 的写法

2. 结合order by对检索结果进行排序，目的是为了使用索引。使用order by后，数据库引擎先对检索列的索引进行从小到大排序并限定了偏移量与索引节点的数量。偏移量大时不适用

3. 将 `limit` 替换为已知位置的查询：使用`where`

   ```sql
   select id, name from t where position between 10 and 20;
   ```

4. 记录上次查询结果的位置，避免使用`offset`：（高效率实现上下翻页）

#### 索引

Hash索引和B+ Tree索引：Hash索引不能做范围查询

聚簇索引（主键索引）和非聚簇索引（非主键索引） → 回表 → 覆盖索引 → 联合索引 → 最左匹配原则

![](http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/4E04B38ED784486286A182B50BDDB3CA/11554)

[MySQL:change buffer](https://www.cnblogs.com/virgosnail/p/10454150.html)

### 事务

[MySQL 中隔离级别 RC 与 RR 的区别](https://www.cnblogs.com/digdeep/p/4968453.html)

1. 事务特性

   **一致性，原子性，隔离性，永久性**

   begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句（第一个快照读语句），事务才真正启动。如果想要马上启动一个事务，可以使用start transaction with consistent snapshot 命令

2. 隔离级别

   | 隔离级别\问题                | 脏读     | 不可重复读 | 幻读     |
   | ---------------------------- | -------- | ---------- | -------- |
   | 读未提交（read uncommitted） | √ 可能   | √ 可能     | √ 可能   |
   | 读已提交（read commited）    | × 不可能 | √ 可能     | √ 可能   |
   | 可重复读（repeatable read）  | × 不可能 | × 不可能   | √ 可能   |
   | 串行化                       | × 不可能 | × 不可能   | × 不可能 |

   **脏读**：可以读取未提交的数据。RC 要求解决脏读；

   **不可重复读**：同一个事务中多次执行同一个select，读取到的数据发生了改变(被其它事务update并且提交)；

   **可重复读**：同一个事务中多次执行同一个select， 读取到的数据没有发生改变(一般使用MVCC实现)；RR隔离级别要求达到可重复读的标准；

   **幻读**：同一个事务中多次执行同一个select，读取到的数据行发生改变。也就是行数减少或者增加了(被其它事务delete/insert并且提交)。SERIALIZABLE要求解决幻读问题；

   **这里一定要区分 不可重复读 和 幻读：**

   不可重复读的重点是**修改**:
   同样的条件的select，你读取过的数据, 再次读取出来发现值不一样了

   幻读的重点在于**新增或者删除**:
   同样的条件的select，第1次和第2次读出来的记录数不一样

   在MySQL里，有两个“视图”的概念：

   - 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view ... ，它的查询方法与表一样。
   - 另一个是InnoDB在实现MVCC时用到的**一致性读视图**，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。

   > **事务的可重复读的能力是怎么实现的？**
   >
   > 1. 可重复读的核心就是一致性读（consistent read）；
   > 2. 而事务更新数据只能用当前读（更新数据都是先读后写，这个读，只能读当前的值，称为“当前读”（current read））。
   > 3. 如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

   > **读已提交和可重复读的区别**
   >
   > 1. 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；（查询只承认在事务启动前就已经提交完成的数据）
   > 2. 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图（查询只承认在语句启动前就已经提交完成的数据）

3. MVCC

   1. 所谓的MVCC（Multi-Version Concurrency Control， 多版本并发控制） 指的就是在使用read commited、 repeatable read这两种隔离级别的事务在执行普通的select操作时访问记录的版本链的过程， 这样子可以使不同事务的读-写、 写-读操作并发执行， 从而提升系统性能 
   2. read commited、 repeatable read这两个隔离级别的一个很大不同就是： **生成ReadView的时机不同**， read commited在每一次进行普通select操作前都会生成一个ReadView， 而repeatable read只在第一次进行普通select操作前生成一个ReadView， 之后的查询操作都重复使用这个ReadView 
   3. 通过undo log多版本链条，加上开启事务时候生产的一个ReadView，然后再有一个查询的时候，根据ReadView进行判断的机制，就知道应该读取哪个版本的数据。 
   4. 每条数据都有两个隐藏字段，一个是trx_id，一个是roll_pointer，trx_id是最近一次更新这条数据的事务id，roll_pointer指向更新这个事务之前生成的undo log
   5. 执行一个事务的时候，会生成一个ReadView
      - 若自己的事务id < ReadView中的最大事务id值，去undolog版本链中寻找最近一次更新这条数据的事务id
      - 若自己的事务id > ReadView中的最大事务id值，则使用自己事务id

   > **事务如何实现的MVCC？**
   > 
   > 1. 每个事务都有一个事务ID，叫做transaction id（严格递增）
   > 2. 事务在启动时，找到已提交的最大事务ID记为up_limit_id。
   > 3. 事务在更新一条语句时，比如id=1改为了id=2，会把id=1和该行之前的row trx_id写到undo log里，
   >    并且在数据页上把id的值改为2，并且把修改这条语句的transaction id记在该行行头
   > 4. 一个事务要查看一条数据时，必须先用该事务的up_limit_id与该行的transaction id做比对，如果up_limit_id>=transaction id，那么可以看；如果up_limit_id<transaction id，则只能去undo log里去取。去undo log查找数据的时候，也需要做比对，必须up_limit_id>transaction id，才返回数据


### 锁

1. 全局锁（对整个数据库实例加锁）

   MySQL加全局读锁的命令是 Flush tables with read lock (FTWRL)。加全局锁后，数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句会被阻塞

   **全局锁的典型使用场景是，做全库逻辑备份**。MySQL逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。由于MVCC的支持，这个过程中数据可以正常更新

   > **single-transaction方法只适用于所有的表使用事务引擎（比如InnoDB）的库**
   >
   > 不支持事务的引擎（MyISAM），备份只能通过FTWRL方法
   >
   > **不推荐使用set global readonly=true**方法。
   >
   > - 在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库
   > - 如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁。将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态

2. 表级别锁：表锁，元数据锁（meta data lock，MDL)

   表锁：

   - **意向共享锁**（事务想要获得一张表中某几行的共享锁）

   - **意向排它锁**（事务想要获得一张表中某几行的排他锁）

   - **语法是 lock tables … read/write。**可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放

   **MDL**：（MySQL 5.5版本中引入）

   - MDL不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性。
   - 对一个表做增删改查操作的时候，加MDL读锁；
   - 当要对表做结构变更操作的时候，加MDL写锁。

   > 事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。在做表结构变更的时候，一定要小心不要导致锁住线上查询和更新。
   >
   > **如何安全地给小表加字段**
   >
   > - 要做DDL变更的表刚好有长事务在执行，要考虑先暂停DDL，或者kill掉这个长事务
   > - 要变更的表是一个请求频繁的热点表，在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。

3. 行锁：

   读锁（S锁，共享锁，允许事务读一行数据）

   写锁（X锁，排它锁，允许事务更新或删除一行数据）

   在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是**两阶段锁协议**。

   **InnoDB有三种行锁的算法：**

   - Record Lock：单个行记录上的锁。

   - Gap Lock：间隙锁，锁定一个范围，但不包括记录本身。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。
   - Next-Key Lock：1+2，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题

4. 死锁

   避免死锁：

   > 通过表级锁来减少死锁产生的概率；
   >
   > 多个程序尽量约定以相同的顺序访问表（这也是解决并发理论中哲学家就餐问题的一种思路）；
   >
   > 同一个事务尽可能做到一次锁定所需要的所有资源。
   >
   > 控制并发度：数据库服务端做并发控制。可以考虑将一行改成逻辑上的多行来减少锁冲突，需要根据业务逻辑做详细设计。

   解决死锁：超时机制，InnoDB使用等待图（包括锁的信息链表，事务等待链表）机制进行死锁检测

   > 超时机制：直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout（默认50s）来设置。
   >
   > 死锁检测（主要使用的方法）：发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。

### 日志

1. redolog（重做日志）

   MySQL中的redolog使用了WAL（Write-Ahead Logging）技术，**关键点就是先写日志，再写磁盘**。**redo log是InnoDB引擎特有的日志**。

   InnoDB依赖redo log实现**crash-safe**，即保证即使数据库发生异常重启，之前提交的记录都不会丢失。

   当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。

   InnoDB的redo log是固定大小的。比如可以配置为一组4个文件，每个文件的大小是1GB，那么redolog总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写。

   <img src="https://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/A1A4A3EF177F463EBAE0A503AAED32CA/9447" alt="image" style="zoom:50%;" />
   
   - write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。


   - checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。


   - 如果write pos追上checkpoint，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。

2. binlog（归档日志）

   - binlog是MySQL的Server层的日志，所有引擎都可以使用。
   - redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。
   - binlog可以追加写入。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

3. undolog（回滚日志）

   为了实现事务的原子性， InnoDB存储引擎在实际进行增、 删、 改一条记录时， 都需要先把对应的undo日志记下来。 一般每对一条记录做一次改动， 就对应着一条undo日志 。undolog日志形成一条版本链

### 读写分离

1. MySQL 读写分离的实现

   基于主从复制架构，一个主库，挂多个从库，单单只是写主库，然后主库会自动把数据给同步到从库上去。

2. MySQL 主从复制原理

   主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。

   接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。

   ![](http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/WEBRESOURCEf08d784a6646fa3378b31cacc6b1d3d3/9838)

3. MySQL 主从同步的延时问题

   以前线上确实处理过因为主从同步延时问题而导致的线上的 bug，属于小型的生产事故。

   是这个么场景。有个同学是这样写代码逻辑的。先插入一条数据，再把它查出来，然后更新这条数据。在生产环境高峰期，写并发达到了 2000/s，这个时候，主从复制延时大概是在小几十毫秒。线上会发现，每天总有那么一些数据，我们期望更新一些重要的数据状态，但在高峰期时候却没更新。用户跟客服反馈，而客服就会反馈给我们。

   我们通过 MySQL 命令：

   ```
   show status
   ```

   查看 `Seconds_Behind_Master` ，可以看到从库复制主库的数据落后了几 ms。

   一般来说，如果主从延迟较为严重，有以下解决方案：

   - 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。
   - 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。
   - 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。
   - 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询**设置直连主库**。**不推荐**这种方法，你要是这么搞，读写分离的意义就丧失了。



## 7.数据结构与算法



## 8.Redis

#### 数据结构和使用场景

| 类型               | 编码                | 备注                                                         |
| ------------------ | ------------------- | ------------------------------------------------------------ |
| 字符串(String)     | int，raw，embstr    | 普通的 set 和 get，做简单的 KV 缓存                          |
| 有序列表(List)     | ziplist，linkedlist | 通过 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类<br />通过 lrange 命令，读取某个闭区间内的元素<br />实现简单的消息队列 |
| 哈希对象(Hash)     | ziplist，hashtable  | 可存储结构化的数据，比如一个对象（无对象嵌套）               |
| 集合对象(Set)      | intset，hashtable   | 无序集合，自动去重<br />基于 set 可以实现交集、并集、差集的操作 |
| 有序集合对象(ZSet) | ziplist，skiplist   | 排序的 set，去重但可以排序                                   |

#### 线程模型

基于Reactor模式的文件事件处理器，文件事件处理器的结构包含 4 个部分：

- 多个 socket
- IO 多路复用程序（编译时自动选择系统中性能最高的I/O多路复用函数库）
- 文件事件分派器
- 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）

多个 socket 并发产生不同的操作，每个操作对应不同的文件事件， IO 多路复用程序监听多个 socket，将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，根据 socket 的事件类型交给对应的事件处理器进行处理。

![](http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/2D15E716EF694B46B5CE5ED24C3AAEAB/11667)

#### 过期删除机制，淘汰机制

Redis 过期策略是：**定期删除+惰性删除**。

**定期删除**：Redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。

**惰性删除**：定期删除可能会导致很多过期 key 到了时间并没有被删除掉，这种情况下走惰性删除。即获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。

**内存淘汰机制**：如果定期删除漏掉了很多过期 key，然后也没及时去查，也就没走惰性删除，如果大量过期 key 堆积在内存里，有可能会导致 Redis 内存块耗尽，此时走内存淘汰机制

#### 持久化

RDB：RDB 持久化机制，是对 Redis 中的数据执行**周期性**的持久化，

AOF：AOF 机制对每条写入命令作为日志，以 `append-only` 的模式写入一个日志文件中，在 Redis 重启的时候，可以通过**回放** AOF 日志中的写入指令来重新构建整个数据集。

1. RDB

   - SAVE命令会阻塞Redis服务器进程，直到RDB文件创建完毕
   - BGSAVE命令会派生出一个子进程，由子进程执行磁盘 IO 操作负责创建RDB文件，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。

   优缺点：

   - RDB 会生成多个数据文件，**非常适合做冷备**
   - 使用子进程执行磁盘 IO 操作来进行 RDB 持久化，性能高
   - 相对于AOF，基于 RDB 数据文件来重启和恢复更快
   - 故障时丢数据比AOF多
   - 子进程生成RDB快照文件时，如果文件很大，可能会导致服务暂停数毫秒甚至数秒

2. AOF

   - AOF持久化分为三个步骤：命令追加（append），文件写入，文件同步（sync）
   - 服务器在执行完一个写命令后，将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾，之后再定期写入并同步到AOF文件
   - AOF重写可以产生一个与原AOF文件一直但体积更小的新AOF文件，去掉了冗余命令，由子进程进行AOF重写
   - 在执行BGREWRITEAOF命令时，Redis服务器会维护一个AOF重写缓冲区。从创建子进程开始，服务器执行的写命令会被同时写入AOF写缓冲区和AOF重写缓冲区。AOF重写完成后，子进程向父进程发信号，由父进程追加重写缓冲区的命令，最后再替换掉旧的AOF文件

   优缺点：

   - AOF 可以**更好的保护数据不丢失**，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次 `fsync` 操作，最多丢失 1 秒钟的数据
   - 以 `append-only` 模式写入，**写入性能非常高**，而且文件不容易破损
   - AOF 日志文件过大也不影响客户端读写
   - **AOF 日志文件通常比 RDB 数据快照文件更大**
   - AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低

#### 主从复制

新版主从复制，使用PSYNC命令，具有完整重同步（`full resynchronization`）和部分重同步（`partial resynchronization`）两种模式

完整重同步：

- master 执行 bgsave ，在本地生成一份 rdb 快照文件，然后将rdb文件发送给 slave node
- master node 在生成 rdb 时，会将所有新的写命令缓存在内存中，在 slave node 保存了 rdb 之后，再将新的写命令复制给 slave node。
- slave node 接收到 rdb 之后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，同时**基于旧的数据版本**对外提供服务。
- 如果 slave node 开启了 AOF，那么会立即执行 BGREWRITEAOF，重写 AOF。

部分重同步：

通过复制偏移量，复制积压缓冲区，服务器运行ID实现

- slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 直接从自己的 backlog 中获取部分丢失的数据，复制给 slave  
- master 根据 slave 发送的 psync 中的 offset 来从 backlog 中获取数据

#### 集群

1. 槽指派：集群中的16384个槽可以分别指派给集群中的各个节点。

2. 在集群中执行命令

   节点在接到一个命令请求时，会先检查这个命令请求要处理的键所在的槽是否由自己负责。如果不是的话，节点将向客户端返回一个MOVED错误，MOVED错误携带的信息可以指引客户端转向至正在负责相关槽的节点

3. ASK错误

   如果节点A正在迁移 i 至节点B，那么节点A没能在自己的数据库中找到命令指定的数据库键时，节点A会向客户端返回一个ASK错误，指引客户端到节点B继续查找指定的数据库键

#### ==**数据库和缓存一致性**==

[分布式之数据库和缓存双写一致性方案解析](https://www.cnblogs.com/rjzheng/p/9041659.html)

[如何保证缓存与数据库的双写一致性？](https://gitee.com/Doocs/advanced-java/blob/master/docs/high-concurrency/redis-consistence.md)

三种缓存更新策略：

- 先更新数据库，再更新缓存
- 先删除缓存，再更新数据库
- 先更新数据库，再删除缓存

1. 先更新数据库，再更新缓存（普遍反对的方案）

   **原因一（线程安全角度）**

   同时有请求A和请求B进行更新操作，那么会出现
   （1）线程A更新了数据库
   （2）线程B更新了数据库
   （3）线程B更新了缓存
   （4）线程A更新了缓存
   这就出现请求A更新缓存应该比请求B更新缓存早才对，但是因为网络等原因，B却比A更早更新了缓存。这就导致了脏数据

   **原因二（业务场景角度）**

   有如下两点：
   （1）如果你是一个写数据库场景比较多，而读数据场景比较少的业务需求，采用这种方案就会导致，数据压根还没读到，缓存就被频繁的更新，浪费性能。
   （2）如果你写入数据库的值，并不是直接写入缓存的，而是要经过一系列复杂的计算再写入缓存。那么，每次写入数据库后，都再次计算写入缓存的值，无疑是浪费性能的。显然，删除缓存更为适合。

2. 先删除缓存，再更新数据库（适用于并发度不高的场景）

   该方案会导致不一致的原因是，同时有一个请求A进行更新操作，另一个请求B进行查询操作。那么会出现如下情形:

   （1）请求A进行写操作，删除缓存
   （2）请求B查询发现缓存不存在
   （3）请求B去数据库查询得到旧值
   （4）请求B将旧值写入缓存
   （5）请求A将新值写入数据库

   上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。

   **解决的方法是采用延时双删策略**，伪代码如下

   ```java
   public void write(String key,Object data){
   	  redis.delKey(key);
   	  db.updateData(data);
   	  Thread.sleep(1000);
   	  redis.delKey(key);
   }
   ```

   转化为中文描述就是

   （1）先淘汰缓存

   （2）再写数据库（这两步和原来一样）

   （3）休眠1秒，再次淘汰缓存

   这么做，可以将1秒内所造成的缓存脏数据，再次删除。

   **那么，这个1秒怎么确定的，具体该休眠多久呢？**

   针对上面的情形，需要自行评估自己的项目的读数据业务逻辑的耗时。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。

   数据库采用读写分离架构的解决方案参见[分布式之数据库和缓存双写一致性方案解析](https://www.cnblogs.com/rjzheng/p/9041659.html)

3. 先更新数据库，再删除缓存

   最经典的缓存+数据库读写的模式，Cache Aside Pattern。

   - **失效**：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后放到缓存中，同时返回响应。
   - **命中**：应用程序从cache中取数据，取到后返回。
   - **更新**：先把数据存到数据库中，成功后，再让缓存失效（删除缓存）。

   **为什么是删除缓存，而不是更新缓存？**

   1. 在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。

   2. 更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于**比较复杂的缓存数据计算的场景**，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，**这个缓存到底会不会被频繁访问到？**

      举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有**大量的冷数据**。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。**用到缓存才去算缓存。**

      其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有懒加载思想。

   该策略出现数据库和缓存不一致的两种解决方法：

   1. 维护一个内存队列的方式，做异步串行化。（适用于需要强一致性的场景）参见[如何保证缓存与数据库的双写一致性？](https://gitee.com/Doocs/advanced-java/blob/master/docs/high-concurrency/redis-consistence.md)
   2. 借助消息队列订阅binlog。参见[分布式之数据库和缓存双写一致性方案解析](

#### 缓存并发竞争

每个系统获取分布式锁，确保同一时间，只能有一个系统实例（抢到锁）在操作某个 key，别人都不允许读和写。同时，写入时，比较当前数据的时间戳和缓存中数据的时间戳。

要写入缓存的数据，都是从 mysql 里查出来的，都得写入 mysql 中，写入 mysql 中的时候必须保存一个时间戳，从 mysql 查出来的时候，时间戳也查出来。

每次要**写之前，先判断**一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。

#### 缓存雪崩、穿透、击穿

1. 缓存雪崩
   - 事前：Redis 高可用，主从+哨兵，Redis cluster，避免全盘崩溃。
   - 事中：本地 ehcache 缓存 + hystrix 限流&降级，避免 MySQL 被打死
   - 事后：Redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。

2. 缓存穿透

   - 在接口层增加校验，比如用户鉴权校验，参数做校验，不合法的参数直接代码Return，比如：id 做基础校验，id <=0的直接拦截等。

   - 每次系统 A 从数据库中只要没查到，就写一个空值到缓存里去，比如 `set -999 UNKNOWN` 。然后设置一个过期时间，这样的话，下次有相同的 key 来访问的时候，在缓存失效之前，都可以直接从缓存中取数据。
   - 布隆过滤器：利用高效的数据结构和算法快速判断出你这个Key是否在数据库中存在，不存在你return就好了，存在你就去查了DB刷新KV再return。

3. 缓存击穿
   - 若缓存的数据是基本不会发生更新的，则可尝试将该热点数据设置为永不过期。
   - 若缓存的数据更新不频繁，且缓存刷新的整个流程耗时较少的情况下，则可以采用基于 Redis、zookeeper 等分布式中间件的分布式互斥锁，或者**本地互斥锁**以**保证仅少量的请求能请求数据库并重新构建缓存**，其余线程则在锁释放后能访问到新缓存。（读到空缓存时，拿到锁的线程读数据库并更新缓存，没拿到锁也没读到缓存的线程sleep100ms再重新读缓存）
   - 若缓存的数据更新频繁或者在缓存刷新的流程耗时较长的情况下，可以利用定时线程在缓存过期前**主动地重新构建缓存或者延后缓存的过期时间**，以保证所有的请求能一直访问到对应的缓存。
   - ==问题：使用互斥锁的情况下，Redis挂了怎么办？==

## 9.消息队列

Kafka最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。一个 topic 的数据，是**分散放在多个机器上的，每个机器就放一部分数据**。

#### 高可用机制

Kafka高可用机制：**replica（复制品） 副本机制**

1. 每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。
2. 所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。
3. 写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。

#### 消息队列的推拉模式

在RocketMQ里，consumer被分为2类：MQPullConsumer和MQPushConsumer，其实本质都是拉模式（pull），即consumer轮询从broker拉取消息。
区别是：
push方式里，consumer把轮询过程封装了，并注册MessageListener监听器，取到消息后，唤醒MessageListener的consumeMessage()来消费，对用户而言，感觉消息是被推送过来的。

pull方式里，取消息的过程需要用户自己写，首先通过打算消费的Topic拿到MessageQueue的集合，遍历MessageQueue集合，然后针对每个MessageQueue批量取消息，一次取完后，记录该队列下一次要取的开始offset，直到取完了，再换另一个MessageQueue。

#### 消息不被重复消费

Kafka 有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，**每隔一段时间**（定时定期），会把自己消费过的消息的 offset 提交一下

思路：

- 数据要写库，先根据主键查一下，如果这数据已经存在了，就不插入了，update 
- 写 Redis，每次都是 set，天然幂等性
- 生产者发送每条数据的时候，里面加一个全局唯一的 id，消费者这里消费到了之后，先根据这个 id 去比如 Redis 里查一下。如果没有消费过就处理，然后这个 id 写 Redis。如果消费过了，就不处理
- 基于数据库的唯一键来保证重复数据不会重复插入多条

#### 消息可靠性

1. 消费者丢失数据

   **关闭自动提交** offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢

2. Kafka丢数据

   - 给 topic 设置 `replication.factor` 参数：这个值必须大于 1，要求**每个 partition 必须有至少 2 个副本**。
   - 在 Kafka 服务端设置 `min.insync.replicas` 参数：这个值必须大于 1，这个是要求**一个 leader 至少感知到有一个 follower 还跟自己保持联系**
   - 在 producer 端设置 `acks=all` ：这个是要求每条数据，必须是**写入所有 replica 之后，才能认为是写成功了**
   - 在 producer 端设置 `retries=MAX` （很大很大很大的一个值，无限次重试的意思）：这个是**要求一旦写入失败，就无限重试**

3. 生产者丢数据

   设置了 `acks=all` ，生产者一定不会丢数据

#### 消息顺序性

写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性

#### ==消息队列数据一致性==

1. ==本地消息表==

   

2. **可靠消息最终一致性方案**。直接基于 MQ 来实现事务。比如阿里的 RocketMQ 就支持消息事务。大概的意思就是：

   1. A 系统先发送一个 prepared 消息到 mq，如果这个 prepared 消息发送失败那么就直接取消操作别执行了；
   2. 如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉 mq 发送确认消息，如果失败就告诉 mq 回滚消息；
   3. 如果发送了确认消息，那么此时 B 系统会接收到确认消息，然后执行本地的事务；
   4. mq 会自动**定时轮询**所有 prepared 消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认的消息，是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，而确认消息却发送失败了。
   5. 这个方案里，要是系统 B 的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如 B 系统本地回滚后，想办法通知系统 A 也回滚；或者是发送报警由人工来手工回滚和补偿。
   6. 这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你就用 RocketMQ 支持的，要不你就自己基于类似 ActiveMQ？RabbitMQ？自己封装一套类似的逻辑出来，总之思路就是这样子的。

   ![](http://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/WEBRESOURCE58370c15e031b6fc2fec848e1df264e1/11731)
#### 消息积压和过期失效

消息过期失效：消息丢弃，然后写代码批量重导

大量消息在MQ 里积压了几个小时了还没解决

临时紧急扩容，具体操作步骤和思路如下：

- 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。
- 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。
- 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，**消费之后不做耗时的处理**，直接均匀轮询写入临时建立好的 10 倍数量的 queue。
- 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。
- 等快速消费完积压数据之后，**得恢复原先部署的架构**，**重新**用原先的 consumer 机器来消费消息。

==更多的Kafka问题查看夸克浏览器书签==

## 10.Dubbo

服务治理：调用链路自动生成、服务访问压力以及时长统计、服务分层（避免循环依赖）、调用链路失败监控和报警、服务鉴权、每个服务可用性的监控（接口调用成功率）

服务降级

失败重试和超时重试

## 11.分布式事务、分布式锁



## 12.限流、熔断

限流组件：Guava RateLimiter，==**令牌桶算法**==，Hystrix



## 13.项目相关

QPS提升，如何做到的

线上问题是如何排查的

负载过高，如何排查

## 14.微博Feed流系统设计